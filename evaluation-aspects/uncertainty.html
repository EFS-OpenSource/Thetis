<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Uncertainty consistency (calibration) &mdash; Thetis User Guide</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="../_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="../_static/css/bootstrap3/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" type="text/css" />

<link rel="stylesheet" href="../_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="../_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/text.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/toggle.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    '../',
            VERSION:     '0.2.2',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/javascript" src="../_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="../_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="../_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="top" title="Thetis User Guide" href="../index.html" />
    <link rel="up" title="Key evaluation aspects" href="../aspects.html" />
    <link rel="next" title="Prediction performance of the model" href="performance.html" />
    <link rel="prev" title="Key evaluation aspects" href="../aspects.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../index.html">Thetis User Guide</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
              <li class="dropdown visible-xs">
                <a role="button" id="localToc" data-toggle="dropdown" data-target="#" href="#">Table Of Contents <b class="caret"></b></a>
                <ul class="dropdown-menu localtoc sp-localtoc" role="menu" aria-labelledby="localToc">
                <ul>
<li><a class="reference internal" href="#">Uncertainty consistency (calibration)</a><ul>
<li><a class="reference internal" href="#classification">Classification</a></li>
<li><a class="reference internal" href="#regression-probabilistic">Regression (probabilistic)</a></li>
<li><a class="reference internal" href="#object-detection">Object detection</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

                </ul>
              </li>

            
              <li><a href="../aspects.html" title="Key evaluation aspects" accesskey="P">previous </a></li>
              <li><a href="performance.html" title="Prediction performance of the model" accesskey="N">next </a></li>
              <li><a href="../genindex.html" title="General Index" accesskey="I">index </a></li>
              <li><a href="../aspects.html" accesskey="U">Key evaluation aspects</a></li>
            
            <li class="visible-xs"><a href="../_sources/evaluation-aspects/uncertainty.rst.txt" rel="nofollow">Show Source</a></li>

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="../search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">

      <!-- row -->
      <div class="row">
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
<h3><a href="../index.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Usage examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Preparing input data</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../aspects.html">Key evaluation aspects</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Uncertainty consistency (calibration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance.html">Prediction performance of the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_evaluation.html">Data quality</a></li>
<li class="toctree-l2"><a class="reference internal" href="fairness.html">Fairness</a></li>
<li class="toctree-l2"><a class="reference internal" href="robustness.html">Robustness (coming soon)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../subscription.html">Subscription and pricing</a></li>
</ul>

<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="../search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <section id="uncertainty-consistency-calibration">
<span id="uncertainty"></span><h1>Uncertainty consistency (calibration)<a class="headerlink" href="#uncertainty-consistency-calibration" title="Permalink to this heading">¶</a></h1>
<p>AI models typically make decisions for each input, regardless of their level of uncertainty. In safety-critical
scenarios, it is crucial for the model to provide a reliable self-assessment of its uncertainty. This allows for
appropriate fallback solutions to be applied when the model is unsure of its decision.</p>
<p>Thetis analyzes and evaluates the quality and consistency of the uncertainty that is estimated and attached to each
AI prediction in most cases (e.g., score or confidence information in classification, “objectness” score in object
detection, or variance estimation in probabilistic regression settings).
The evaluation of uncertainty quality depends on the selected task. We give a brief overview about the mathematical
background and the used metrics in the following section.</p>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this heading">¶</a></h2>
<p>Given an input <span class="math notranslate nohighlight">\(X \in \mathcal{X}\)</span>, an AI model predicts a label <span class="math notranslate nohighlight">\(\hat{Y} \in \mathcal{Y}\)</span> targeting
the ground truth label <span class="math notranslate nohighlight">\(\bar{Y} \in \mathcal{Y}\)</span>.
For binary classification, the set of labels is <span class="math notranslate nohighlight">\(\mathcal{Y} \in \{0, 1\}\)</span> and the AI model commonly outputs
a (probability) score <span class="math notranslate nohighlight">\(\hat{P} \in [0, 1]\)</span>, denoting the confidence in predicting the positive class.
The uncertainty estimation of a binary classification model is <em>well calibrated</em>, if</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(\bar{Y} = 1 | \hat{P} = \hat{p}) = \hat{p}, \quad \forall \hat{p} \in [0, 1] ,\]</div>
<p>is fulfilled <span id="id1">[<a class="reference internal" href="#id22" title="Mahdo Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2901–2907. 2015.">NCH15</a>]</span>, <span id="id2">[<a class="reference internal" href="#id21" title="Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 1321–1330. PMLR, August 2017.">GPSW17</a>]</span>, <span id="id3">[<a class="reference internal" href="#id27" title="Fabian Küppers. Uncertainty Calibration and its Application to Object Detection. PhD thesis, University of Wuppertal, School of Mathematics and Natural Sciences, 2023.">Kup23</a>]</span>. For example, consider 100 predictions of an AI model,
each one with a confidence score of 80%. If we observe an accuracy of 80% as well, the AI model is <em>well-calibrated</em>.</p>
<p>In multi-class classification settings (<span class="math notranslate nohighlight">\(\mathcal{Y} \in \{1, \ldots, K\}\)</span> with <span class="math notranslate nohighlight">\(K\)</span> classes), the
definition for uncertainty calibration is similar to the binary classification case by</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(\hat{Y} = \bar{Y} | \hat{P} = \hat{p}) = \hat{p}, \quad \forall \hat{p} \in [0, 1] .\]</div>
<p>Measuring calibration can be done using several metrics:</p>
<p><strong>Expected Calibration Error (ECE):</strong> the ECE can be derived from the definition for calibrated classification
uncertainty and is a common metric to measure miscalibration <span id="id4">[<a class="reference internal" href="#id22" title="Mahdo Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2901–2907. 2015.">NCH15</a>]</span>. The target is to measure the
expectation of the difference between predicted confidence and observed accuracy <span id="id5">[<a class="reference internal" href="#id22" title="Mahdo Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2901–2907. 2015.">NCH15</a>]</span>,
<span id="id6">[<a class="reference internal" href="#id21" title="Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural Networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 1321–1330. PMLR, August 2017.">GPSW17</a>]</span>, <span id="id7">[<a class="reference internal" href="#id27" title="Fabian Küppers. Uncertainty Calibration and its Application to Object Detection. PhD thesis, University of Wuppertal, School of Mathematics and Natural Sciences, 2023.">Kup23</a>]</span>, which is denoted by</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{\hat{P} \sim f_{\hat{P}}} \Big[ | \mathbb{P}(\hat{Y} = \bar{Y} | \hat{P} = \hat{p}) - \hat{p} | \Big].\]</div>
<p>Since the expectation is given over a continuous distribution, it is directly tractable.
Instead, the ECE is an approximation of the expectation by applying a binning scheme over the confidence space.
Each predicted sample is grouped into a bin by its confidence. Afterwards, we can compare the average accuracy with
the average confidence within each bin to compute the <em>conditional</em> probability of correctness.
Thus, the ECE is given by</p>
<div class="math notranslate nohighlight">
\[\text{ECE} := \sum^{B}_{b=1} \frac{N_b}{N} | \text{acc}(b) - \text{conf}(b) | ,\]</div>
<p>with <span class="math notranslate nohighlight">\(B\)</span> bins, <span class="math notranslate nohighlight">\(N\)</span> samples within the evaluation dataset, <span class="math notranslate nohighlight">\(N_b\)</span> samples within a single bin, and with
<span class="math notranslate nohighlight">\(\text{acc}(b)\)</span> and <span class="math notranslate nohighlight">\(\text{conf}(b)\)</span> as the accuracy and average confidence within bin <span class="math notranslate nohighlight">\(b\)</span>, respectively.</p>
<p><strong>Maximum Calibration Error (MCE):</strong> the MCE is highly related to the ECE but denotes the <em>maximum</em> difference
between average confidence and accuracy over the binning scheme <span id="id8">[<a class="reference internal" href="#id22" title="Mahdo Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2901–2907. 2015.">NCH15</a>]</span>. Thus, the MCE is given by</p>
<div class="math notranslate nohighlight">
\[\text{MCE} := \max_{b \in \{1, \ldots, B\}} | \text{acc}(b) - \text{conf}(b) | .\]</div>
<p><strong>Negative Log Likelihood (NLL):</strong> the NLL can be interpreted as a proper scoring rule that estimates the deviation
between a predicted probability distribution and the true distribution. Thus, the NLL function is also commonly
used as a complementary uncertainty metric.</p>
<p><strong>Brier Score:</strong> similar to the NLL, the Brier Score is also a proper scoring rule and is given by the
mean squared error of the forecast. It is also commonly used as a complementary calibration metric.</p>
<p><strong>Homogeneity Rating:</strong> one of the rating aspects is the homogeneity of the calibration error. Thetis assigns a
higher rating if the calibration error is constant across the different levels of confidence, i.e., when the
variance of the calibration error for different confidence levels is low.</p>
<p><strong>Outlier Rating:</strong> a further rating aspect is the presence/absence of confidence intervals with a significantly
higher calibration error compared to the average level of miscalibration.</p>
</section>
<section id="regression-probabilistic">
<h2>Regression (probabilistic)<a class="headerlink" href="#regression-probabilistic" title="Permalink to this heading">¶</a></h2>
<p>In the context of regression (estimation of a continuous target score), several machine learning/deep learning
algorithms are able to output an estimation uncertainty along with the predicted score.
Similar to the classification case, this uncertainty shall reflect the (expected) prediction error. If the AI
model consistently over- or underestimates the prediction error, it is considered to be <em>miscalibrated</em>.</p>
<p>Mathematically, a (probabilistic) AI model takes an (arbitrary) input and predicts a
mean <span class="math notranslate nohighlight">\(\mu_\hat{R} \in \mathbb{R}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2_\hat{R} \in \mathbb{R}_{&gt;0}\)</span>,
so that the random variable for the model predictions is given by
<span class="math notranslate nohighlight">\(\hat{R} \sim \mathcal{N}(\mu_\hat{R}, \sigma^2_\hat{R})\)</span>, targeting the ground truth
score <span class="math notranslate nohighlight">\(\bar{R} \in \mathbb{R}\)</span>.</p>
<p>There exist several definitions for the term uncertainty calibration in the context of
probabilistic regression <span id="id9">[<a class="reference internal" href="#id26" title="Fabian Küppers, Jonas Schneider, and Anselm Haselhoff. Parametric and multivariate uncertainty calibration for regression and object detection. In European Conference on Computer Vision Workshops. Springer, October 2022. in press.">KSH22</a>]</span>. The commonly used definition is
<em>quantile calibration</em> where it is required that the estimated prediction intervals for a certain quantile level
<span class="math notranslate nohighlight">\(\tau \in (0, 1)\)</span> cover <span class="math notranslate nohighlight">\(\tau%\)</span> of the ground truth scores of a validation dataset.</p>
<p>More formally, let <span class="math notranslate nohighlight">\(\hat{F}_{\hat{R}}: \mathbb{R} \rightarrow (0, 1)\)</span> be the predicted
cumulative distribution function (CDF) of <span class="math notranslate nohighlight">\(\hat{R}\)</span> and the (inverse) percent point
function (PPF), i.e., the quantile function be given by
<span class="math notranslate nohighlight">\(\hat{F}_{\hat{R}}^{-1}: (0, 1) \rightarrow \mathbb{R}\)</span> accordingly.
A prediction model is quantile calibrated, if</p>
<div class="math notranslate nohighlight">
\[\mathbb{P} \Big( \bar{R} \leq \hat{F}_{\hat{R}}^{-1}(\tau) \Big) = \tau, \quad \forall \tau \in (0, 1) ,\]</div>
<p>is fulfilled.</p>
<p>Several metrics exist to evaluate for <em>quantile calibration</em>:</p>
<p><strong>Quantile loss aka Pinball loss</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{L}_{\text{Pin}}(\tau) =
\begin{cases}
    (\bar{r} - \hat{F}_{\hat{R}}^{-1}(\tau))\tau \quad &amp;\text{if } \bar{r} \geq \hat{F}_{\hat{R}}^{-1}(\tau) \\
    (\hat{F}_{\hat{R}}^{-1}(\tau) - \bar{r})(1 - \tau) \quad &amp;\text{if } \bar{r} &lt; \hat{F}_{\hat{R}}^{-1}(\tau)
\end{cases}\end{split}\]</div>
<p><strong>Quantile Calibration Error</strong>:
The quantile calibration error measures the absolute difference between expected quantile level and actual quantile
coverage of the ground truth scores by the predicted distributions for a certain quantile level
<span id="id10">[<a class="reference internal" href="#id26" title="Fabian Küppers, Jonas Schneider, and Anselm Haselhoff. Parametric and multivariate uncertainty calibration for regression and object detection. In European Conference on Computer Vision Workshops. Springer, October 2022. in press.">KSH22</a>]</span>. For a dataset with <span class="math notranslate nohighlight">\(N\)</span> samples, the QCE is given by</p>
<div class="math notranslate nohighlight">
\[QCE(\tau) := \Bigg| \frac{1}{N} \sum^N_{n=1} \mathbb{1}(\epsilon_{\bar{R}_n} \leq \chi^2(\tau)) - \tau \Bigg| ,\]</div>
<p>where <span class="math notranslate nohighlight">\(\chi^2(\tau)\)</span> denotes the PPF of the Chi-Square distribution with 1 degree of freedom.
Furthermore, <span class="math notranslate nohighlight">\(\epsilon_{\bar{R}_n}\)</span> denotes the Normalized Estimation Error Squared (NEES),
also known as the squared Mahalanobis distance, which is given for the one-dimensional case by</p>
<div class="math notranslate nohighlight">
\[\epsilon_{\bar{R}_n} :=  \frac{(\bar{R} - \mu_{\hat{R}})^2}{\sigma^2_{\hat{R}}} .\]</div>
<p><strong>Proper scoring rules: Negative Log Likelihood (NLL)</strong>:
The NLL for continuous random variables is given by</p>
<div class="math notranslate nohighlight">
\[NLL := - \frac{1}{N} \sum^N_{n=1} \log \Big(f_{\hat{R}}(\bar{r}; \mu_{\hat{R}}, \sigma^2_{\hat{R}})\Big) ,\]</div>
<p>with <span class="math notranslate nohighlight">\(f_{\hat{R}}(\bar{r}; \mu_{\hat{R}}, \sigma^2_{\hat{R}})\)</span> as the probability density function (PDF)
of the predicted distribution at ground truth score <span class="math notranslate nohighlight">\(\bar{r}\)</span>.
The NLL can be used as a metric to evaluate the uncertainty calibration properties of an estimator since
it captures the goodness of fit of probability distributions. It jointly measures the baseline
prediction performance as well as the quality of the uncertainty estimates.</p>
</section>
<section id="object-detection">
<h2>Object detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h2>
<p>In contrast to classification, the task of (image-based) object detection is to estimate the presence and location
of multiple objects within a single image. Thus, object detection is the joint task of (semantic) classification and
(spatial) regression.</p>
<p>For the uncertainty calibration evaluation of the <strong>semantic classification</strong> output (e.g., objectness score), we can
use the same metrics as for the standard classification uncertainty calibration evaluation.
The uncertainty evaluation differs from standard classification evaluation in two ways:</p>
<ol class="arabic simple">
<li><p>Since most applications do not have access to the <em>true negatives</em> (correctly identified background as such), it is
not possible to calculate the accuracy. Thus, the calibration target is the precision <span id="id11">[<a class="reference internal" href="#id23" title="Fabian Küppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 326–327. 2020.">KKSH20</a>]</span>.</p></li>
<li><p>For the computation of the precision, it is necessary to match predicted objects with real existing (ground truth)
objects. However, this matching strategy depends on the selected Intersection over Union (IoU) score. The specified
IoU describes to which degree predicted and existing objects need to overlap to be considered as matching. Thus,
all evaluation results are given w.r.t. a certain IoU score.</p></li>
</ol>
<p>Furthermore, recent work has shown that the calibration error might also be position-dependent
<span id="id12">[<a class="reference internal" href="#id23" title="Fabian Küppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 326–327. 2020.">KKSH20</a>]</span>, <span id="id13">[<a class="reference internal" href="#id24" title="Fabian Küppers, Anselm Haselhoff, Jan Kronenberger, and Jonas Schneider. Confidence Calibration for Object Detection and Segmentation, pages 225–250. Springer International Publishing, Cham, 2022. URL: https://doi.org/10.1007/978-3-031-01233-4_8, doi:10.1007/978-3-031-01233-4_8.">KHKS22</a>]</span>, i.e., the calibration properties of objects located in the center
of an image might differ from objects located at the image boundaries.
Thus, given an object detection model that estimates an object with label <span class="math notranslate nohighlight">\(\hat{Y} \in \mathcal{Y}\)</span>,
confidence <span class="math notranslate nohighlight">\(\hat{P} \in [0, 1]\)</span>, and position information <span class="math notranslate nohighlight">\(\hat{\mathbf{R}} \in \mathcal{R}\)</span>,
<em>position-dependent</em> calibration is defined by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{P}(\hat{M} = 1 | \hat{P} = \hat{p}, \hat{Y} = \hat{y}, \hat{\mathbf{R}} = \hat{\mathbf{r}}) = \hat{p}, \\
\forall \hat{p} \in [0, 1], \hat{y} \in \mathcal{Y}, \hat{\mathbf{r}} \in \mathcal{R} ,\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{M}\)</span> evaluates to <span class="math notranslate nohighlight">\(1\)</span> if the predicted object matches a real existing (ground truth) object.</p>
<p><strong>Detection Expected Calibration Error (D-ECE):</strong> from this definition, we can derive the D-ECE similar as to the ECE.
The target is to minimize the position-dependent expectation of the difference between predicted
confidence and observed precision. The D-ECE is an approximation by applying a multi-dimensional binning scheme over
the joint confidence, label, and position space <span id="id14">[<a class="reference internal" href="#id23" title="Fabian Küppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 326–327. 2020.">KKSH20</a>]</span>, <span id="id15">[<a class="reference internal" href="#id24" title="Fabian Küppers, Anselm Haselhoff, Jan Kronenberger, and Jonas Schneider. Confidence Calibration for Object Detection and Segmentation, pages 225–250. Springer International Publishing, Cham, 2022. URL: https://doi.org/10.1007/978-3-031-01233-4_8, doi:10.1007/978-3-031-01233-4_8.">KHKS22</a>]</span>, and is given by</p>
<div class="math notranslate nohighlight">
\[\text{D-ECE} := \sum^{B}_{b=1} \frac{N_b}{N} | \text{prec}(b) - \text{conf}(b) | ,\]</div>
<p>with <span class="math notranslate nohighlight">\(B\)</span> bins, <span class="math notranslate nohighlight">\(N\)</span> samples within the evaluation dataset, <span class="math notranslate nohighlight">\(N_b\)</span> samples within a single bin, and with
<span class="math notranslate nohighlight">\(\text{prec}(b)\)</span> and <span class="math notranslate nohighlight">\(\text{conf}(b)\)</span> as the precision and average confidence within bin <span class="math notranslate nohighlight">\(b\)</span>, respectively.</p>
<p>For the uncertainty calibration evaluation of the <strong>spatial regression</strong> output (uncertainty for bounding box position),
we simply adapt the methods used for uncertainty calibration evaluation in the context of (probabilistic)
regression <span id="id16">[<a class="reference internal" href="#id26" title="Fabian Küppers, Jonas Schneider, and Anselm Haselhoff. Parametric and multivariate uncertainty calibration for regression and object detection. In European Conference on Computer Vision Workshops. Springer, October 2022. in press.">KSH22</a>]</span>, <span id="id17">[<a class="reference internal" href="#id27" title="Fabian Küppers. Uncertainty Calibration and its Application to Object Detection. PhD thesis, University of Wuppertal, School of Mathematics and Natural Sciences, 2023.">Kup23</a>]</span>.</p>
<p>Similar to the evaluation of classification uncertainty, Thetis also applies a rating for <strong>Homogeneity</strong> and
<strong>Outliers</strong> in the context of object detection uncertainty evaluation.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id18">
<div class="citation" id="id21" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>GPSW17<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural Networks. In <em>Proceedings of the 34th International Conference on Machine Learning</em>, volume 70 of Proceedings of Machine Learning Research, 1321–1330. PMLR, August 2017.</p>
</div>
<div class="citation" id="id27" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Kup23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id7">2</a>,<a role="doc-backlink" href="#id17">3</a>)</span>
<p>Fabian Küppers. <em>Uncertainty Calibration and its Application to Object Detection</em>. PhD thesis, University of Wuppertal, School of Mathematics and Natural Sciences, 2023.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KHKS22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id15">2</a>)</span>
<p>Fabian Küppers, Anselm Haselhoff, Jan Kronenberger, and Jonas Schneider. <em>Confidence Calibration for Object Detection and Segmentation</em>, pages 225–250. Springer International Publishing, Cham, 2022. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-031-01233-4_8">https://doi.org/10.1007/978-3-031-01233-4_8</a>, <a class="reference external" href="https://doi.org/10.1007/978-3-031-01233-4_8">doi:10.1007/978-3-031-01233-4_8</a>.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KKSH20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id12">2</a>,<a role="doc-backlink" href="#id14">3</a>)</span>
<p>Fabian Küppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidence calibration for object detection. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em>, 326–327. 2020.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>KSH22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id10">2</a>,<a role="doc-backlink" href="#id16">3</a>)</span>
<p>Fabian Küppers, Jonas Schneider, and Anselm Haselhoff. Parametric and multivariate uncertainty calibration for regression and object detection. In <em>European Conference on Computer Vision Workshops</em>. Springer, October 2022. in press.</p>
</div>
<div class="citation" id="id22" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>NCH15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id5">3</a>,<a role="doc-backlink" href="#id8">4</a>)</span>
<p>Mahdo Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. In <em>Proceedings of the 29th AAAI Conference on Artificial Intelligence</em>, 2901–2907. 2015.</p>
</div>
</div>
</div>
</section>
</section>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="../index.html">Thetis User Guide</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="../aspects.html" title="Key evaluation aspects" >previous</a></li>
        <li><a href="performance.html" title="Prediction performance of the model" >next</a></li>
        <li><a href="../genindex.html" title="General Index" >index</a></li>
        <li><a href="../aspects.html" >Key evaluation aspects</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>