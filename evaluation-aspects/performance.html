<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Prediction performance of the model &mdash; Thetis User Guide</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="../_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="../_static/css/bootstrap3/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" type="text/css" />

<link rel="stylesheet" href="../_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="../_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/text.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/toggle.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    '../',
            VERSION:     '0.2.2',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/javascript" src="../_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="../_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="../_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="top" title="Thetis User Guide" href="../index.html" />
    <link rel="up" title="Key evaluation aspects" href="../aspects.html" />
    <link rel="next" title="Data quality" href="data_evaluation.html" />
    <link rel="prev" title="Uncertainty consistency (calibration)" href="uncertainty.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../index.html">Thetis User Guide</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
              <li class="dropdown visible-xs">
                <a role="button" id="localToc" data-toggle="dropdown" data-target="#" href="#">Table Of Contents <b class="caret"></b></a>
                <ul class="dropdown-menu localtoc sp-localtoc" role="menu" aria-labelledby="localToc">
                <ul>
<li><a class="reference internal" href="#">Prediction performance of the model</a><ul>
<li><a class="reference internal" href="#classification">Classification</a></li>
<li><a class="reference internal" href="#regression">Regression</a></li>
<li><a class="reference internal" href="#object-detection">Object detection</a></li>
</ul>
</li>
</ul>

                </ul>
              </li>

            
              <li><a href="uncertainty.html" title="Uncertainty consistency (calibration)" accesskey="P">previous </a></li>
              <li><a href="data_evaluation.html" title="Data quality" accesskey="N">next </a></li>
              <li><a href="../genindex.html" title="General Index" accesskey="I">index </a></li>
              <li><a href="../aspects.html" accesskey="U">Key evaluation aspects</a></li>
            
            <li class="visible-xs"><a href="../_sources/evaluation-aspects/performance.rst.txt" rel="nofollow">Show Source</a></li>

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="../search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">

      <!-- row -->
      <div class="row">
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
<h3><a href="../index.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Usage examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Preparing input data</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../aspects.html">Key evaluation aspects</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="uncertainty.html">Uncertainty consistency (calibration)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Prediction performance of the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_evaluation.html">Data quality</a></li>
<li class="toctree-l2"><a class="reference internal" href="fairness.html">Fairness</a></li>
<li class="toctree-l2"><a class="reference internal" href="robustness.html">Robustness (coming soon)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../subscription.html">Subscription and pricing</a></li>
</ul>

<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="../search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <section id="prediction-performance-of-the-model">
<span id="performance"></span><h1>Prediction performance of the model<a class="headerlink" href="#prediction-performance-of-the-model" title="Permalink to this heading">¶</a></h1>
<p>Thetis provides basic metrics that are commonly used for the AI evaluation process.</p>
<p><strong>Important:</strong> Since the rating and evaluation of the baseline AI performance highly depend on the actual use case,
Thetis does not provide a rating score or recommendations on the AI performance metrics.</p>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this heading">¶</a></h2>
<p>For binary and multi-class classification tasks, we compute several metrics which are commonly used
to evaluate the baseline performance of an AI algorithm. As the basis, we refer to the terms
<em>true positives</em> (TP, correctly identified samples with a positive label), <em>true negatives</em> (TN, correctly identified
samples with a negative label), <em>false positives</em> (FP, samples with a predicted positive label but true negative label),
and <em>false negatives</em> (FN, samples with a predicted negative label but true positive label).
In a multi-class scenario, if a metric is based on TP, TN, FP, or FN (e.g., precision or recall), we apply
macro-averaging, i.e., the metrics are computed for each class separately, using a one-vs-all scheme, and averaged
afterwards to yield single scores.
Currently, the following metrics are implemented:</p>
<ul class="simple">
<li><p><strong>Accuracy</strong> directly estimates the probability that the model classifies an instance correctly for a given dataset. While this metric is intuitive, it depends on the base rate of occurrence for each class in the dataset and is not applicable if the base rates in the dataset do not represent the class distribution in the classifier’s deployment domain (see <a class="reference external" href="https://en.wikipedia.org/wiki/Base_rate_fallacy">Wikipedia article about base rate fallacy</a>).</p></li>
<li><p><strong>Sensitivity</strong>, also known as recall, quantifies the ability of the classifier to capture all positive instances correctly. It measures the proportion of true positive instances that are correctly identified out of all actual positive instances. Sensitivity is important for minimizing false negatives, ensuring that positive instances are not overlooked.</p></li>
<li><p><strong>Precision</strong> measures the accuracy of positive predictions, indicating the proportion of true positive predictions out of all positive predictions made by the classifier. It focuses on minimizing false positives, crucial in scenarios where incorrectly identifying positive instances carries significant consequences.</p></li>
<li><p><strong>Matthews correlation coefficient (MCC)</strong> is a metric used to evaluate the performance where different aspects are gathered within a single score. The metric ranges from -1 to +1, where a score of +1 indicates perfect predictions with no mistakes. A score of 0 suggests that the classifier’s predictions are no better than random guessing, while a score of -1 indicates total disagreement between predictions and actual labels, with all predictions being incorrect.</p></li>
</ul>
<p>In the context of binary classification, we also report the following metrics:</p>
<ul class="simple">
<li><p><strong>Specificity</strong> complements sensitivity by measuring the classifier’s ability to avoid false alarms in identifying negative instances. It quantifies the proportion of true negative instances that are correctly identified out of all actual negative instances. High specificity minimizes false positive errors, enhancing the classifier’s ability to accurately identify negative instances.</p></li>
<li><p><strong>Negative predictive value</strong> assesses the reliability of negative predictions made by the classifier. It quantifies the proportion of true negative predictions out of all negative predictions made by the classifier. Negative predictive value is particularly relevant in scenarios where correctly identifying negative instances is crucial.</p></li>
<li><p><strong>Informedness</strong> measures the classifier’s ability to make correct positive and negative predictions simultaneously. Informedness ranges from -1 to 1, where a score of 1 indicates perfect classification, 0 indicates random classification, and -1 indicates perfectly incorrect classification.</p></li>
<li><p><strong>Markedness</strong> evaluates the classifier’s ability to correctly predict positive instances while avoiding false positives. Markedness also ranges from -1 to 1, with 1 representing perfect classification, 0 indicating random classification, and -1 representing perfect misclassification.</p></li>
</ul>
<p>Although a computation of <strong>specificity</strong> and <strong>negative predictive value</strong> would also be possible in a multiclass scenario
(as averages over all given classes, similar to recall and precision), we report these metrics for binary classification only.
In binary classification, another expression for specificity and NPV would be “recall and precision for the negative class”.
Hence, a comprehensive evaluation of the classifier must consider both values, so as not to ignore the negative class.
In a multiclass setting, recall and precision already cover the classifier’s performance over all classes as macro-average.
We therefore deem the addition of specificity and NPV redundant for multiclass tasks.</p>
<p>When used in combination, these metrics provide a comprehensive evaluation of classifier performance. Precision and sensitivity focus on the performance of positive predictions, while specificity and negative predictive value assess the performance of negative predictions. By considering both positive and negative predictions, these metrics offer insights into different aspects of classifier performance, enabling informed decision-making in various applications.</p>
</section>
<section id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this heading">¶</a></h2>
<p>For regression tasks (estimation of a continuous target score), several metrics about the prediction performance
(estimation error) are computed as basic indicators to assess the model performance depending on the
actual use case.
The following metrics are currently implemented:</p>
<ul class="simple">
<li><p>Mean Absolute Error - mean of absolute errors over all samples.</p></li>
<li><p>Median Absolute Error - median of absolute errors over all samples. This metric is more robust against outliers.</p></li>
<li><p>Mean Squared Error - mean of squared errors over all samples.</p></li>
<li><p>Root Mean Squared Error - root mean of squared errors over all samples.</p></li>
<li><p>Mean Absolute Percentage Error - mean of absolute errors relative to the absolute target score. The idea of this
metric is to denote the error relative to the target data range. However, note that this metric is not interpretable
as a percentage score in a <span class="math notranslate nohighlight">\([0, 100]\)</span> interval and can be arbitrarily high when the target scores are small and
the prediction error is large.</p></li>
<li><p>Coefficient of Determination (R2) - also known as <span class="math notranslate nohighlight">\(R^2\)</span> score which denotes the estimation error relative to
the variance in the target scores. For more information, see
<a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Coefficient of Determination</a>.</p></li>
</ul>
<p>Furthermore, a diagram about predicted vs. target scores as well as a diagram about the distribution of
residuals is also shown to give a visual representation of the estimation error.</p>
</section>
<section id="object-detection">
<h2>Object detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h2>
<p>In the context of object detection, we utilize the following metrics:</p>
<ul class="simple">
<li><p>Precision - fraction of all correctly predicted objects (TP) over all predicted objects (TP and FP), so that <span class="math notranslate nohighlight">\(\text{TP} / (\text{TP} + \text{FP})\)</span>.</p></li>
<li><p>Recall - fraction of all correctly identified objects (TP) over all real existing objects (TP and FN), so that <span class="math notranslate nohighlight">\(\text{TP} / (\text{TP} + \text{FN})\)</span>.</p></li>
<li><p>Average Precision (AP) - the AP score is defined as the area under the precision-recall curve for a varying confidence threshold.</p></li>
<li><p>F1 score - harmonic mean of the precision and recall with <span class="math notranslate nohighlight">\(2 * (\text{precision} * \text{recall}) / (\text{precision} + \text{recall})\)</span></p></li>
</ul>
</section>
</section>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="../index.html">Thetis User Guide</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="uncertainty.html" title="Uncertainty consistency (calibration)" >previous</a></li>
        <li><a href="data_evaluation.html" title="Data quality" >next</a></li>
        <li><a href="../genindex.html" title="General Index" >index</a></li>
        <li><a href="../aspects.html" >Key evaluation aspects</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>