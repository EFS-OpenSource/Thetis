<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Fairness &mdash; Thetis User Guide</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="../_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="../_static/css/bootstrap3/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" type="text/css" />

<link rel="stylesheet" href="../_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="../_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/text.css" type="text/css" />
<link rel="stylesheet" href="../_static/css/toggle.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    '../',
            VERSION:     '0.2.2',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/javascript" src="../_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="../_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="../_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="top" title="Thetis User Guide" href="../index.html" />
    <link rel="up" title="Key evaluation aspects" href="../aspects.html" />
    <link rel="next" title="Robustness (coming soon)" href="robustness.html" />
    <link rel="prev" title="Data quality" href="data_evaluation.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../index.html">Thetis User Guide</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
              <li class="dropdown visible-xs">
                <a role="button" id="localToc" data-toggle="dropdown" data-target="#" href="#">Table Of Contents <b class="caret"></b></a>
                <ul class="dropdown-menu localtoc sp-localtoc" role="menu" aria-labelledby="localToc">
                <ul>
<li><a class="reference internal" href="#">Fairness</a><ul>
<li><a class="reference internal" href="#classification">Classification</a></li>
<li><a class="reference internal" href="#regression">Regression</a><ul>
<li><a class="reference internal" href="#performance-fairness">Performance Fairness</a></li>
<li><a class="reference internal" href="#prediction-fairness">Prediction Fairness</a></li>
</ul>
</li>
<li><a class="reference internal" href="#object-detection">Object detection</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

                </ul>
              </li>

            
              <li><a href="data_evaluation.html" title="Data quality" accesskey="P">previous </a></li>
              <li><a href="robustness.html" title="Robustness (coming soon)" accesskey="N">next </a></li>
              <li><a href="../genindex.html" title="General Index" accesskey="I">index </a></li>
              <li><a href="../aspects.html" accesskey="U">Key evaluation aspects</a></li>
            
            <li class="visible-xs"><a href="../_sources/evaluation-aspects/fairness.rst.txt" rel="nofollow">Show Source</a></li>

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="../search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">

      <!-- row -->
      <div class="row">
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
<h3><a href="../index.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Usage examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Preparing input data</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../aspects.html">Key evaluation aspects</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="uncertainty.html">Uncertainty consistency (calibration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance.html">Prediction performance of the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_evaluation.html">Data quality</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Fairness</a></li>
<li class="toctree-l2"><a class="reference internal" href="robustness.html">Robustness (coming soon)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../subscription.html">Subscription and pricing</a></li>
</ul>

<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="../search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <section id="fairness">
<span id="id1"></span><h1>Fairness<a class="headerlink" href="#fairness" title="Permalink to this heading">¶</a></h1>
<p>The evaluation of fairness of an AI algorithm is highly important to assess for a possible discrimination of
individual sensitive groups. Thetis evaluates for <em>group fairness</em> which evaluates the risk for individual groups of
experiencing discrimination. The fairness rating of an AI model is based on several fairness metrics which are
described in the following.</p>
<section id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this heading">¶</a></h2>
<p>Given an input <span class="math notranslate nohighlight">\(X \in \mathcal{X}\)</span>, a classification model <span class="math notranslate nohighlight">\(h(X)\)</span> makes predictions
<span class="math notranslate nohighlight">\(\hat{Y} \in \mathcal{Y}\)</span> targeting the ground truth label <span class="math notranslate nohighlight">\(\bar{Y} \in \mathcal{Y}\)</span>, so that
<span class="math notranslate nohighlight">\(h: \mathcal{X} \rightarrow \mathcal{Y}\)</span>. Furthermore, let <span class="math notranslate nohighlight">\(A \in \mathcal{A}\)</span> denote a sensitive attribute
(e.g., attribute <em>gender</em>) for each <span class="math notranslate nohighlight">\(X\)</span>. For baseline classifier evaluation, we can determine
the <em>true positives</em> (TP, correctly identified samples with positive label), <em>true negatives</em> (TN, correctly identified
samples with negative label), <em>false positives</em> (FP, samples with a predicted positive label but true negative label),
and <em>false negatives</em> (FN, samples with predicted negative label but true positive label). From these scores, it is possible
to obtain the metrics <em>sensitivity (recall)</em> and <em>precision</em>, as well as <em>specificity</em> and <em>negative predictive value</em>
for binary classification tasks, respectively.</p>
<p>Now we want to compare the classifier performance by means of sensitive attribute <span class="math notranslate nohighlight">\(A\)</span> and evaluate if there exists
any disparity in performance for different labels of attribute <span class="math notranslate nohighlight">\(A\)</span> (e.g., a possible performance disparity
for labels <em>male</em>/<em>female</em> of attribute <em>gender</em>). The comparison is achieved by computing the aforementioned metrics
for each label separately, which is denoted by <span class="math notranslate nohighlight">\(\text{sens}_a\)</span>, <span class="math notranslate nohighlight">\(\text{spec}_a\)</span>,
<span class="math notranslate nohighlight">\(\text{prec}_a\)</span>, and <span class="math notranslate nohighlight">\(\text{neg}_a\)</span> in the following for sensitivity, specificity, precision, and
negative predictive value, respectively.</p>
<p><strong>Minimum ratio</strong>: For each of these metrics, Thetis reports the minimum ratio between the group with lowest and
highest score, grouped by the labels of <span class="math notranslate nohighlight">\(A\)</span>, which is denoted by</p>
<div class="math notranslate nohighlight">
\[\frac{\min_{a \in \mathcal{A}} \text{metric}_a}
{\max_{a \in \mathcal{A}} \text{metric}_a} ,\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{metric}_a\)</span> represents the considered metric.</p>
<p>Inspecting the ratio between minimum and maximum value quantifies the <strong>relative</strong> disparity between
individual groups. Using ratios instead of absolute differences allows us to handle cases with low baseline performance. For example, consider sensitive
attribute <em>gender</em> with labels <em>male</em> and <em>female</em> and a classifier that yields a sensitivity of
<span class="math notranslate nohighlight">\(0.025\)</span> for the label <em>female</em> and <span class="math notranslate nohighlight">\(0.05\)</span> for the label <em>male</em>. While the absolute difference between
sensitivity scores is only <span class="math notranslate nohighlight">\(0.025\)</span>, the classifier still achieves twice the sensitivity for the label <em>male</em>.</p>
<p><strong>Maximum difference</strong>: In contrast, when the <strong>absolute</strong> disparity between individual groups is of interest, Thetis
also reports the maximum difference between the metric with highest score and the metric with lowest score,
computed for each label of <span class="math notranslate nohighlight">\(A\)</span> separately, which is denoted by</p>
<div class="math notranslate nohighlight">
\[\max_{a \in \mathcal{A}} \text{metric}_a - \min_{a \in \mathcal{A}} \text{metric}_a ,\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{metric}_a\)</span> represents the considered metric.</p>
<p><strong>Metric interpretation</strong>: The ratios/differences for each of the aforementioned metrics can be interpreted as follows:</p>
<ul class="simple">
<li><p><strong>Sensitivity (Recall)</strong>: Large relative or absolute differences in sensitivity indicate that the classifier performs better in recognizing positive samples for some sensitive groups than for other groups. This may be due to learned imbalances in the dataset, a general lack of training data for some groups, or an inherent correlation between sensitive groups and classification difficulty.</p></li>
<li><p><strong>Precision</strong>: A significant relative or absolute difference in precision is an indicator that <em>positive</em> predictions of the classifier are less likely to be correct for some sensitive groups than for others. This may be due to a general lack of training data for some groups, an inherent correlation between sensitive groups and classification difficulty, or a difference in base class prevalence depending on the sensitive group.</p></li>
</ul>
<p>The following metrics are only reported in the case of binary classification:
* <strong>Specificity</strong>: A disparity of specificity is an indicator that the classifier tends to produce more false alarms for certains groups of sensitive attributes, which indicates a disparity of performance in distinguishing negative instances.
* <strong>Negative predictive value</strong>: A significant relative or absolute difference in the negative predictive value is an indicator that <em>negative</em> predictions of the classifier are less likely to be correct for some sensitive groups than for others. Similar to precision disparity, possible causes are a general lack of training data for some groups, an inherent correlation between sensitive groups and classification difficulty, or a difference in base class prevalence depending on the sensitive group.</p>
</section>
<section id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this heading">¶</a></h2>
<p>In the context of regression fairness evaluation, we work with continuous target
scores <span class="math notranslate nohighlight">\(\bar{R} \in \mathcal{R} = \mathbb{R}\)</span> as well as continuous prediction scores
<span class="math notranslate nohighlight">\(\hat{R} \in \mathcal{R} = \mathbb{R}\)</span>.
We further work with categorical sensitive attributes <span class="math notranslate nohighlight">\(A \in \mathcal{A}\)</span>
(e.g., gender, race, etc.) with <span class="math notranslate nohighlight">\(\mathcal{A} = \{1, \ldots, L\}\)</span>.
We distinguish between <em>Performance Fairness</em> (parity of prediction performance, i.e., estimation error) and
<em>Prediction Fairness</em> (parity of model predictions) which is explained in more detail below.</p>
<section id="performance-fairness">
<h3>Performance Fairness<a class="headerlink" href="#performance-fairness" title="Permalink to this heading">¶</a></h3>
<p>The goal is to determine if the model’s prediction performance is equal for all labels within
a sensitive group.</p>
<p><strong>Mean Absolute Error</strong>: The mean absolute error (MAE) between target and prediction
scores is defined by</p>
<div class="math notranslate nohighlight">
\[\text{MAE}_a = \frac{1}{N_a} \sum^{N_a}_{n=1} | \bar{r}_{n, a} - \hat{r}_{n, a} | ,\]</div>
<p>where <span class="math notranslate nohighlight">\(N_a\)</span> is the number of samples where the sensitive attribute is a certain <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span>.
The greater the difference in MAE for different sensitive groups, the more unfair a model is to be considered.</p>
<p><strong>Point Biserial Correlation</strong>: The point biserial correlation coefficient denotes the correlation
between a categorical binary random variable and a continuous one. It is similar to the Pearson correlation
coefficient but for a categorical and continuous random variable, respectively.
Let <span class="math notranslate nohighlight">\(\Delta R = \bar{R} - \hat{R}\)</span> denote the estimation error. Since the biserial correlation
coefficient is only defined for binary random variables, we use a “one-vs-all” scheme to compute a
correlation coefficient for each <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span> separately.
Thus, the point biserial correlation coefficient can be calculated by</p>
<div class="math notranslate nohighlight">
\[\rho_{A, \Delta R} = \frac{(\mu_{\Delta R}^{A=a} - \mu_{\Delta R}^{A \neq a})}{\sigma_{\Delta R}}
\sqrt{\frac{N_{A=a} N_{A \neq a}}{N^2}} ,\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_{\Delta R}^{A=a}\)</span> and <span class="math notranslate nohighlight">\(\mu_{\Delta R}^{A \neq a}\)</span> denote the mean scores of
the estimation error for all samples with label <span class="math notranslate nohighlight">\({A=a}\)</span> and vice versa, whereas
<span class="math notranslate nohighlight">\(\sigma_{\Delta R}\)</span> denotes the standard deviation of the estimation error.
Furthermore, <span class="math notranslate nohighlight">\(N_{A=a}\)</span> and <span class="math notranslate nohighlight">\(N_{A \neq a}\)</span> denote the amount of samples with
label <span class="math notranslate nohighlight">\({A=a}\)</span> and vice versa.</p>
<p><strong>Separation</strong>: This aspect of fairness requires statistical independence between model predictions and
the sensitive attribute given the target scores, so that</p>
<div class="math notranslate nohighlight">
\[\hat{R} \perp A | \bar{R} \rightarrow f(\hat{R}, A | \bar{R}) =  f(\hat{R}| \bar{R})f(A| \bar{R}) .\]</div>
<p>This metric is sensitive to a possible bias in the target scores itself so that it is useful to solely
quantify the performance of the underlying AI model <span id="id2">[<a class="reference internal" href="#id7" title="Daniel Steinberg, Alistair Reid, and Simon O'Callaghan. Fairness measures for regression via probabilistic classification. In 2nd Ethics of Data Science Conference. March 2020.">SROCallaghan20</a>]</span>.</p>
<p><strong>Sufficiency</strong>: This aspect of fairness requires statistical independence between target scores and the sensitive
attribute given the predicted scores, so that</p>
<div class="math notranslate nohighlight">
\[\bar{R} \perp A | \hat{R} \rightarrow f(\bar{R}, A | \hat{R}) =  f(\bar{R}| \hat{R})f(A| \hat{R}) .\]</div>
<p>While <strong>separation</strong> deals with parity of error rates for similar individuals,
sufficiency focuses on error parity among individuals who are given the same decision <span id="id3">[<a class="reference internal" href="#id7" title="Daniel Steinberg, Alistair Reid, and Simon O'Callaghan. Fairness measures for regression via probabilistic classification. In 2nd Ethics of Data Science Conference. March 2020.">SROCallaghan20</a>]</span>.
Mathematically, this distinction is similar to the one between precision and recall.</p>
</section>
<section id="prediction-fairness">
<h3>Prediction Fairness<a class="headerlink" href="#prediction-fairness" title="Permalink to this heading">¶</a></h3>
<p>In the previous section, we describe the fairness evaluation for the estimation error.
However, it is also possible to quantify the fairness/equality of the model predictions itself.
In case that the underlying dataset itself is biased, e.g., towards a certain sensitive attribute,
this will also be reflected by the model predictions if the estimation error is constant for all sensitive groups.
However, this might rather indicate a bias within the data itself instead of a bias within the trained AI model.
Thus, the application of prediction fairness may depend on the use case and what is intended for the current
examination.</p>
<p><em>Note</em>: Since the goal is to quantify the fairness of the underlying AI model itself, the following metrics for
<em>Prediction Fairness</em> are not considered for the final fairness rating. Currently, only aspects of
<em>Performance Fairness</em> are considered for the fairness rating.</p>
<p><strong>Point Biserial Correlation</strong>: The point biserial correlation coefficient can also be computed for the
model predictions <span class="math notranslate nohighlight">\(\hat{R}\)</span> itself so that the correlation coefficient is denoted by <span class="math notranslate nohighlight">\(\rho_{A, \hat{R}}\)</span>.
When the data evaluation of Thetis is active, it will also compute the correlation coefficient between the sensitive
attributes and the target scores <span class="math notranslate nohighlight">\(\bar{R}\)</span> which yields the correlation coefficient <span class="math notranslate nohighlight">\(\rho_{A, \bar{R}}\)</span>.
This is useful to determine if a correlation is induced by the data itself or the used AI model.</p>
<p><strong>Independence</strong>: This aspect requires the equality of predictions for all sensitive attributes which
can be expressed by</p>
<div class="math notranslate nohighlight">
\[\hat{R} \perp A \rightarrow f(\hat{R}, A) =  f(\hat{R})f(A) .\]</div>
<p>Note that a possible bias in the target values has an influence to the computation of the independence score.
Thus, it is a metric for both, a bias in the target values as well as a bias in the model
prediction performance <span id="id4">[<a class="reference internal" href="#id7" title="Daniel Steinberg, Alistair Reid, and Simon O'Callaghan. Fairness measures for regression via probabilistic classification. In 2nd Ethics of Data Science Conference. March 2020.">SROCallaghan20</a>]</span>.</p>
</section>
</section>
<section id="object-detection">
<h2>Object detection<a class="headerlink" href="#object-detection" title="Permalink to this heading">¶</a></h2>
<p>The metrics for evaluating the fairness of object detection algorithms are highly related to the ones used
for classification fairness evaluation. However, we have two limitations in the case of object detection:</p>
<ul class="simple">
<li><p>In a common object detection setting we do not have access to the <em>true negatives</em> (TN, correctly identified background).</p></li>
<li><p>For object predictions with no associated ground-truth information, we commonly do not have information about a sensitive attribute available.</p></li>
</ul>
<p>Therefore, only the computation of the ratio/difference of the <strong>recall</strong> (correctly identified objects as such) is
applicable since the FP used in <em>precision</em> computation may not contain sensitive attribute information, and the
computation of <em>specificity</em> and <em>negative predictive value</em> requires the presence of TN.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id5">
<div class="citation" id="id7" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>SROCallaghan20<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>,<a role="doc-backlink" href="#id4">3</a>)</span>
<p>Daniel Steinberg, Alistair Reid, and Simon O'Callaghan. Fairness measures for regression via probabilistic classification. In <em>2nd Ethics of Data Science Conference</em>. March 2020.</p>
</div>
</div>
</div>
</section>
</section>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="../index.html">Thetis User Guide</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="data_evaluation.html" title="Data quality" >previous</a></li>
        <li><a href="robustness.html" title="Robustness (coming soon)" >next</a></li>
        <li><a href="../genindex.html" title="General Index" >index</a></li>
        <li><a href="../aspects.html" >Key evaluation aspects</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>