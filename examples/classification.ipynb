{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Classification example with Thetis\n",
    "\n",
    "Thetis can evaluate AI systems that perform classification tasks. In this example, we demonstrate how to evaluate and rate an AI model using a basic classification example from [scikit-learn](https://scikit-learn.org/). The instructions below should be easy to adapt to your own use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Set up the environment\n",
    "\n",
    "If you haven't done so already, install Thetis using pip:\n",
    "\n",
    "```shell\n",
    "$ pip install thetis\n",
    "```\n",
    "\n",
    "For this example, you can use the demo license located within the same directory as this notebook.\n",
    "This license only works for our demonstration dataset with the exact configuration provided in this notebook.\n",
    "Use the license file [demo_license_classification.dat](https://raw.githubusercontent.com/EFS-OpenSource/Thetis/main/examples/demo_license_classification.dat).\n",
    "\n",
    "Place the license file either in the working directory of your application or at:\n",
    "\n",
    "- Windows: `<User>/AppData/Local/Thetis/license.dat`\n",
    "- Unix: `~/.local/thetis/license.dat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Increase logging verbosity\n",
    "\n",
    "To obtain detailed runtime information about Thetis, run the following cell. This will add a logging handler to the Thetis logger, increasing the application's verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "# Configure root logger as catch-all logging config\n",
    "logger = logging.getLogger(\"Thetis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prepare example data and model\n",
    "\n",
    "To start with our basic classification example, we need to load some data. In this tutorial, we use the\n",
    "[\"Adult\" dataset](https://www.openml.org/search?type=data&sort=runs&id=179&status=active).\n",
    "This dataset demonstrates a prediction task that determines whether a person earns over 50K a year.\n",
    "Let's load the dataset using tools from the scikit-learn library.\n",
    "\n",
    "*Note:* If your machine is behind a proxy server, downloading the example data as shown here may not work.\n",
    "If that is the case for you, check out our [detection example](detection.ipynb) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "testset_size = 10000\n",
    "\n",
    "# use \"fetch_openml\" by scikit-learn to load \"Adult\" dataset from OpenML\n",
    "dataset, target = fetch_openml(data_id=1590, return_X_y=True, parser=\"auto\")\n",
    "\n",
    "df_train, df_test = dataset.iloc[:-testset_size], dataset.iloc[-testset_size:]\n",
    "target_train, target_test = target.iloc[:-testset_size], target.iloc[-testset_size:]\n",
    "\n",
    "# drop columns with sensitive attributes from classifier input and convert categorical attributes to one-hot\n",
    "df_train_cleared = df_train.drop(columns=[\"education\", \"race\", \"sex\", \"native-country\", \"relationship\", \"marital-status\"])\n",
    "df_test_cleared = df_test.drop(columns=[\"education\", \"race\", \"sex\", \"native-country\", \"relationship\", \"marital-status\"])\n",
    "\n",
    "# convert categorical columns to class codes with integer representation\n",
    "categorical_columns = [\"workclass\", \"occupation\"]\n",
    "df_train_cleared[categorical_columns] = df_train_cleared[categorical_columns].apply(lambda col: pd.Categorical(col).codes)\n",
    "df_test_cleared[categorical_columns] = df_test_cleared[categorical_columns].apply(lambda col: pd.Categorical(col).codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This yields two [Pandas](https://pandas.pydata.org/) data frames with a reduced set of information.\n",
    "\n",
    "In the next step, we train a simple Random Forest classifier on the training data using scikit-learn.\n",
    "We then use the trained model to make predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize a Random Forest classifier and fit to training data\n",
    "classifier = RandomForestClassifier(verbose=True)\n",
    "classifier.fit(pd.get_dummies(df_train_cleared), target_train)\n",
    "\n",
    "# finally, make predictions on the validation dataset\n",
    "confidence = classifier.predict_proba(pd.get_dummies(df_test_cleared))\n",
    "labels = classifier.predict(pd.get_dummies(df_test_cleared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Represent data as pandas DataFrames\n",
    "\n",
    "Thetis expects two Pandas data frames to run an evaluation:\n",
    "\n",
    "* **Annotations**: A `pd.DataFrame` with ground truth information about the dataset. The column `target` is required and should hold the ground truth target information. Additionally, columns for sensitive attributes are expected if they have been configured for fairness evaluation.\n",
    "* **Predictions**: A `pd.DataFrame` with the AI predictions for each sample in the dataset. The columns `labels` and `confidence` are required and should hold information about the predicted label and the respective prediction probability (model uncertainty or confidence).\n",
    "\n",
    "Note that the indices of the data frames for annotations and predictions must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sensitive attributes during safety evaluation\n",
    "annotations = pd.DataFrame({\"target\": target_test, \"race\": df_test[\"race\"], \"sex\": df_test[\"sex\"]})\n",
    "predictions = pd.DataFrame({\"labels\": labels, \"confidence\": confidence[:, 1]}, index=annotations.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Optionally, you can read/write the `pd.DataFrame` instances in CSV format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: store prediction and ground truth data on disk\n",
    "annotations.to_csv(\"adult_annotations.csv\")\n",
    "predictions.to_csv(\"adult_predictions.csv\")\n",
    "\n",
    "# optional: load prediction and ground truth data from disk\n",
    "# important: specify \"index_col\" since Thetis matches the predictions/annotations by their indices\n",
    "loaded_annotations = pd.read_csv(\"adult_annotations.csv\", index_col=0)\n",
    "loaded_predictions = pd.read_csv(\"adult_predictions.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run Thetis to analyze and evaluate the AI system\n",
    "\n",
    "You can download the [demo configuration file](https://raw.githubusercontent.com/EFS-OpenSource/Thetis/main/examples/demo_config_classification.yaml) for this example from our repository. For detailed information on Thetis configuration, refer to the [Configuration](https://efs-opensource.github.io/Thetis/configuration.html) section.\n",
    "\n",
    "In addition to generating the report in PDF format, which we display below, Thetis also returns its findings, final rating, and recommendations for mitigation strategies as a JSON-like dictionary. We capture this dictionary as `result` and access it as follows:\n",
    "\n",
    "* `result[<aspect>]` contains a sub-dictionary with results for each aspect of the analysis, e.g. 'fairness' or 'uncertainty'.\n",
    "* `result[<aspect>]['rating_score']` contains the rating as a score from 0 to 10.\n",
    "* `result[<aspect>]['rating_enum']` contains the rating as a grade, which can be `'GOOD'`, `'MEDIUM'`, or `'BAD'`, depending on the rating score.\n",
    "* `result[<aspect>]['recommendations']` contains findings regarding possible issues and recommendations for mitigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetis import thetis\n",
    "\n",
    "\n",
    "result = thetis(\n",
    "   config=\"demo_config_classification.yaml\",\n",
    "   annotations=annotations,\n",
    "   predictions=predictions,\n",
    "   output_dir=\"./output\",\n",
    "   license_file_path=\"demo_license_classification.dat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"./output/report.pdf\", width=800, height=1024)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
