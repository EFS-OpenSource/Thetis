{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Object Detection Example (Image-based) with Thetis\n",
    "\n",
    "Thetis can evaluate the AI safety of modern, image-based object detector models.\n",
    "In this example, we utilize a [Faster R-CNN model by Torchvision](https://pytorch.org/vision/main/models/faster_rcnn.html) in conjunction\n",
    "with a custom demo data set ([download here](https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip)) to demonstrate the evaluation process for\n",
    "object detectors. The instructions below should be easy do adapt to your own use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Set Up the Environment\n",
    "In a first step, you need to install Thetis by using pip. We also use the TQDM Library to draw nice progress bars:\n",
    "\n",
    "```shell\n",
    "$ pip install thetis tqdm\n",
    "```\n",
    "\n",
    "Next, you need to obtain a license in order to use Thetis.\n",
    "\n",
    "\n",
    "For the current example, you can use the *demo license* located within the same directory as this notebook.\n",
    "This license only works for our demonstration data set with the exact configuration provided in this notebook.\n",
    "Use the license file [demo_license_detection.dat](https://raw.githubusercontent.com/EFS-OpenSource/Thetis/main/examples/demo_license_detection.dat).\n",
    "\n",
    "A customized *full license*, enabling you to run Thetis with your own data sets and settings, is available at our [Subscription Page](https://efs-opensource.github.io/Thetis/subscription.html).\n",
    "\n",
    "Place the license file either in the working directory of your application or at:\n",
    "\n",
    "- Windows: `<User>/AppData/Local/Thetis/license.dat`\n",
    "- Unix: `~/.local/thetis/license.dat`\n",
    "\n",
    "## Install PyTorch\n",
    "To follow through this example, you need an installation of PyTorch. Please follow the installation instructions at [PyTorch Homepage](https://pytorch.org/).\n",
    "\n",
    "You won't need to train a model from scratch in this example. We only adapt a model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Increase Logging Verbosity\n",
    "\n",
    "For detailed runtime information about Thetis, run the following cell to add a logging handler to the Thetis logger to increase verbosity of the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure root logger as catch-all logging config\n",
    "logger = logging.getLogger(\"Thetis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run Inference with the PyTorch Object Detector\n",
    "\n",
    "First, we need to load and initialize our model, the [Faster R-CNN by Torchvision](https://pytorch.org/vision/main/models/faster_rcnn.html).\n",
    "Note that the model is pre-trained on the MS COCO data set with several categories. In our example, we only\n",
    "work with the categories \"person\", \"bicycle\", and \"car\".\n",
    "\n",
    "*Note:* If your machine is behind a proxy, you likely need to configure your environment so that the following code\n",
    "can download the pre-trained model and data set. Set the `HTTP_PROXY` and `HTTPS_PROXY` environment variables to point\n",
    "to your proxy server and port before launching python or jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "# initialize object detection model from torchvision model zoo\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# retrieve necessary image transformations (e.g., normalization, etc.) and available categories\n",
    "preprocess = weights.transforms()\n",
    "categories = np.array(weights.meta[\"categories\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the next step, download and extract\n",
    "the [Demo Detection Data Set](https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip) which is artificially generated using\n",
    "the [Carla simulation engine](https://carla.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# download demo detection data set\n",
    "urllib.request.urlretrieve(\"https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip\", \"thetis_demo_detection.zip\")\n",
    "\n",
    "# unzip data set to local disk\n",
    "with ZipFile(\"thetis_demo_detection.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"demo_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "After download and extraction, we can load the JSON annotation\n",
    "files and run inference with the Torchvision model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# set the inference device - if you have a CUDA device, you can set it to \"cuda:<idx>\"\n",
    "device = \"cpu\"\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# get a list of all JSON files\n",
    "annotation_files = glob(os.path.join(\"demo_detection\", \"annotations\", \"*.json\"))\n",
    "data = []\n",
    "\n",
    "# iterate over all JSON files and retrieve annotations\n",
    "for filename in tqdm(annotation_files, desc=\"Running inference on images ...\"):\n",
    "    with open(filename, \"r\") as open_file:\n",
    "        anns = json.load(open_file)\n",
    "\n",
    "    # load respective image, run preprocessing (transformation) and finally run inference\n",
    "    img = read_image(os.path.join(\"demo_detection\", \"img\", anns[\"image_file\"]), ImageReadMode.RGB)\n",
    "    img = [preprocess(img).to(device=device)]\n",
    "\n",
    "    # make inference and copy back to CPU (if CUDA device has been used for inference)\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)[0]\n",
    "        pred = {k: v.cpu() for k, v in pred.items()}\n",
    "\n",
    "    # store predicted and target data for current frame\n",
    "    data.append((pred, anns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Expected Data Format for Object Detection\n",
    "\n",
    "After loading the ground-truth information and running inference using an AI model (see example above),\n",
    "we must format our predictions and annotations in a way that can be ingested by Thetis. In object detection evaluation mode,\n",
    "Thetis expects a Python dictionary for the predictions and annotations, where the keys represent the image identifiers\n",
    "(e.g., image name) and the values represent the individual (predicted or ground-truth) objects within a single frame.\n",
    "\n",
    "*Important*: The dictionary for the ground-truth annotations requires a key `__meta__` which holds width and height\n",
    "information for each image within the data set, provided as Pandas DataFrame. Note that the index of the entries within\n",
    "this DataFrame must match with the keys (i.e. image identifiers) of the Python dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Thetis expects a dictionary with image name as key and a pd.DataFrame with predicted information as value.\n",
    "# A similar format is also expected for the ground-truth annotations with additional sensitive attributes\n",
    "# used for fairness evaluation. The field \"__meta__\" is always required with meta information for each frame.\n",
    "annotations = {\"__meta__\": pd.DataFrame(columns=[\"width\", \"height\"])}\n",
    "predictions = {}\n",
    "\n",
    "# iterate over all frames with predicted and target information\n",
    "for pred, anns in data:\n",
    "\n",
    "    # retrieve predicted labels, bounding boxes, and filter predictions by label\n",
    "    predicted_labels = categories[pred[\"labels\"].numpy()]\n",
    "    predicted_boxes = pred[\"boxes\"].numpy().reshape((-1, 4))\n",
    "    target_boxes = np.array(anns[\"boxes\"]).reshape((-1, 4))\n",
    "    filter = np.isin(predicted_labels, [\"person\", \"bicycle\", \"car\"])\n",
    "    filename = anns[\"image_file\"]\n",
    "\n",
    "    # add predicted information as pd.DataFrame\n",
    "    predictions[filename] = pd.DataFrame.from_dict({\n",
    "        \"labels\": predicted_labels[filter],\n",
    "        \"confidence\": pred[\"scores\"].numpy()[filter],\n",
    "        \"xmin\": predicted_boxes[:, 0][filter],\n",
    "        \"ymin\": predicted_boxes[:, 1][filter],\n",
    "        \"xmax\": predicted_boxes[:, 2][filter],\n",
    "        \"ymax\": predicted_boxes[:, 3][filter],\n",
    "    })\n",
    "\n",
    "    # add ground-truth information also as pd.DataFrame with additional sensitive attributes\n",
    "    annotations[filename] = pd.DataFrame.from_dict({\n",
    "        \"target\": anns[\"classes\"],\n",
    "        \"gender\": anns[\"gender\"],\n",
    "        \"age\": anns[\"age\"],\n",
    "        \"xmin\": target_boxes[:, 0],\n",
    "        \"ymin\": target_boxes[:, 1],\n",
    "        \"xmax\": target_boxes[:, 2],\n",
    "        \"ymax\": target_boxes[:, 3],\n",
    "    })\n",
    "\n",
    "    # some additional meta information such as image width and height are also required\n",
    "    annotations[\"__meta__\"].loc[filename] = [anns[\"image_width\"], anns[\"image_height\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Optional: Store and Load Data Set to/from Disk\n",
    "\n",
    "The inference routine as well as the data preparation process might be time-consuming. To store the prepared data set on disk, you can utilize the helper function `write_json_with_pandas`. You can re-load the data set from disk using the `read_json_with_pandas` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetiscore.io import write_json_with_pandas, read_json_with_pandas\n",
    "\n",
    "# write the JSON-like Python dictionary to disk\n",
    "write_json_with_pandas(\n",
    "    json_like=predictions,\n",
    "    filename=\"carla_predictions.json\",\n",
    ")\n",
    "\n",
    "write_json_with_pandas(\n",
    "    json_like=annotations,\n",
    "    filename=\"carla_annotations.json\",\n",
    ")\n",
    "\n",
    "# load the dictionaries from disk\n",
    "load_predictions = read_json_with_pandas(\"carla_predictions.json\")\n",
    "load_annotations = read_json_with_pandas(\"carla_annotations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Run AI Safety Evaluation with Thetis\n",
    "\n",
    "Given your data is in the right format, simply call Thetis with the predictions, the ground-truth information and the prepared configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetiscore import thetis\n",
    "\n",
    "\n",
    "result = thetis(\n",
    "   config=\"demo_config_detection.yaml\",\n",
    "   annotations=annotations,\n",
    "   predictions=predictions,\n",
    "   output_dir=\"./output\",\n",
    "   license_file_path=\"demo_license_detection.dat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "For details of Thetis configuration, see section [Configuration](https://efs-opensource.github.io/Thetis/configuration.html).\n",
    "For the current example, you can download the [demo configuration file](https://raw.githubusercontent.com/EFS-OpenSource/Thetis/main/examples/demo_config_detection.yaml)\n",
    "from this repository or [click here](https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip).\n",
    "\n",
    "Thetis returns its findings, the final rating and recommendations for mitigation strategies as a JSON-like dictionary. We capture the dictionary as `result` and can access the different evaluation aspects:\n",
    "\n",
    "* `result[<task>]['rating_score']` for the rating score of the selected task (e.g., 'fairness' or 'uncertainty').\n",
    "* `result[<task>]['recommendations']` for the recommendations to mitigate possible issues of the selected task.\n",
    "* `result[<task>]['rating_enum']` for a categorization of the actual aspect into `'GOOD'`, `'MEDIUM'`,\n",
    "  or `'BAD'` depending on the rating score.\n",
    "\n",
    "Note that the remaining evaluation metrics are grouped by the specified IoU scores which are used for the matching\n",
    "of predicted objects with ground-truth ones (e.g., an IoU score of 0.5 might be used to decide if a prediction\n",
    "has matched an existing ground-truth object or not). In the configuration file, you can specify multiple IoU scores\n",
    "that are taken into account for the final evaluation process."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
