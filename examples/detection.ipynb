{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection example with Thetis\n",
    "\n",
    "Thetis can evaluate AI systems that perform image-based object detection tasks.\n",
    "In this example, we utilize a [Faster R-CNN model by Torchvision](https://pytorch.org/vision/main/models/faster_rcnn.html) in conjunction\n",
    "with a custom demo dataset ([download here](https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip)) to demonstrate the evaluation process for object detectors. The instructions below should be easy do adapt to your own use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "If you haven't done so already, install Thetis using pip. We also use the TQDM library to draw nice progress bars:\n",
    "\n",
    "```shell\n",
    "$ pip install thetis tqdm\n",
    "```\n",
    "\n",
    "For this example, you can use the demo license located within the same directory as this notebook.\n",
    "This license only works for our demonstration dataset with the exact configuration provided in this notebook.\n",
    "Use the license file [demo_license_classification.dat](https://raw.githubusercontent.com/EFS-OpenSource/Thetis/main/examples/demo_license_classification.dat).\n",
    "\n",
    "Place the license file either in the working directory of your application or at:\n",
    "\n",
    "- Windows: `<User>/AppData/Local/Thetis/license.dat`\n",
    "- Unix: `~/.local/thetis/license.dat`\n",
    "\n",
    "## Install PyTorch\n",
    "\n",
    "To follow through this example, you need an installation of PyTorch. Please follow the installation instructions at [PyTorch Homepage](https://pytorch.org/).\n",
    "\n",
    "You won't need to train a model from scratch in this example. We only adapt a model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase logging verbosity\n",
    "\n",
    "To obtain detailed runtime information about Thetis, run the following cell. This will add a logging handler to the Thetis logger, increasing the application's verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure root logger as catch-all logging config\n",
    "logger = logging.getLogger(\"Thetis\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference with the PyTorch object detector\n",
    "\n",
    "First, we need to load and initialize our model, the [Faster R-CNN by Torchvision](https://pytorch.org/vision/main/models/faster_rcnn.html).\n",
    "Note that the model is pre-trained on the MS COCO dataset with several categories. In our example, we only\n",
    "work with the categories \"person\" and \"car\".\n",
    "\n",
    "*Note:* If your machine is behind a proxy, you likely need to configure your environment so that the following code\n",
    "can download the pre-trained model and dataset. Set the `HTTP_PROXY` and `HTTPS_PROXY` environment variables to point\n",
    "to your proxy server and port before launching Python or Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "# initialize object detection model from torchvision model zoo\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# retrieve necessary image transformations (e.g., normalization, etc.) and available categories\n",
    "preprocess = weights.transforms()\n",
    "categories = np.array(weights.meta[\"categories\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, download and extract our [demo detection dataset](https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip)\n",
    "which we artificially generated using the [Carla simulation engine](https://carla.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# download demo detection dataset\n",
    "urllib.request.urlretrieve(\"https://thetishostedfiles.blob.core.windows.net/demofiles/thetis_demo_detection.zip\", \"thetis_demo_detection.zip\")\n",
    "\n",
    "# unzip dataset to local disk\n",
    "with ZipFile(\"thetis_demo_detection.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"demo_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After download and extraction, we can load the JSON annotation\n",
    "files and run inference with the Torchvision model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# set the inference device - if you have a CUDA device, you can set it to \"cuda:<idx>\"\n",
    "device = \"cpu\"\n",
    "# device = \"cuda:0\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# get a list of all JSON files\n",
    "annotation_files = glob(os.path.join(\"demo_detection\", \"annotations\", \"*.json\"))\n",
    "data = []\n",
    "\n",
    "# iterate over all JSON files and retrieve annotations\n",
    "for filename in tqdm(annotation_files, desc=\"Running inference on images ...\"):\n",
    "    with open(filename, \"r\") as open_file:\n",
    "        anns = json.load(open_file)\n",
    "\n",
    "    # load respective image, run preprocessing (transformation) and finally run inference\n",
    "    img = read_image(os.path.join(\"demo_detection\", \"img\", anns[\"image_file\"]), ImageReadMode.RGB)\n",
    "    img = [preprocess(img).to(device=device)]\n",
    "\n",
    "    # make inference and copy back to CPU (if CUDA device has been used for inference)\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)[0]\n",
    "        pred = {k: v.cpu() for k, v in pred.items()}\n",
    "\n",
    "    # store predicted and target data for current frame\n",
    "    data.append((pred, anns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected data format for object detection\n",
    "\n",
    "After loading the ground truth data and running inference using an AI model (see the example above), you need to format your predictions and annotations to be compatible with Thetis. In object detection evaluation mode, Thetis expects predictions and annotations to be provided as Python dictionaries. In these dictionaries, the keys should represent the image identifiers (e.g. image names), and the values should represent the individual objects (predicted or ground truth) within each frame.\n",
    "\n",
    "### Include image metadata\n",
    "\n",
    "The dictionary for ground truth annotations must include a key `__meta__`, which contains the width and height information for each image in the dataset. This information should be provided as a Pandas DataFrame. The index of this DataFrame must match the keys (image identifiers) of the Python dictionaries used for the ground truth annotations and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Thetis expects a dictionary with image name as key and a pd.DataFrame with predicted information as value.\n",
    "# A similar format is also expected for the ground truth annotations with additional sensitive attributes\n",
    "# used for fairness evaluation. The field \"__meta__\" is always required with meta information for each frame.\n",
    "annotations = {\"__meta__\": pd.DataFrame(columns=[\"width\", \"height\"])}\n",
    "predictions = {}\n",
    "\n",
    "# iterate over all frames with predicted and target information\n",
    "for pred, anns in data:\n",
    "\n",
    "    # retrieve predicted labels, bounding boxes, and filter predictions by label\n",
    "    predicted_labels = categories[pred[\"labels\"].numpy()]\n",
    "    predicted_boxes = pred[\"boxes\"].numpy().reshape((-1, 4))\n",
    "    target_boxes = np.array(anns[\"boxes\"]).reshape((-1, 4))\n",
    "    filter = np.isin(predicted_labels, [\"person\", \"car\"])\n",
    "    filename = anns[\"image_file\"]\n",
    "\n",
    "    # add predicted information as pd.DataFrame\n",
    "    predictions[filename] = pd.DataFrame.from_dict({\n",
    "        \"labels\": predicted_labels[filter],\n",
    "        \"confidence\": pred[\"scores\"].numpy()[filter],\n",
    "        \"xmin\": predicted_boxes[:, 0][filter],\n",
    "        \"ymin\": predicted_boxes[:, 1][filter],\n",
    "        \"xmax\": predicted_boxes[:, 2][filter],\n",
    "        \"ymax\": predicted_boxes[:, 3][filter],\n",
    "    })\n",
    "\n",
    "    # add ground truth information also as pd.DataFrame with additional sensitive attributes\n",
    "    annotations[filename] = pd.DataFrame.from_dict({\n",
    "        \"target\": anns[\"classes\"],\n",
    "        \"gender\": anns[\"gender\"],\n",
    "        \"age\": anns[\"age\"],\n",
    "        \"xmin\": target_boxes[:, 0],\n",
    "        \"ymin\": target_boxes[:, 1],\n",
    "        \"xmax\": target_boxes[:, 2],\n",
    "        \"ymax\": target_boxes[:, 3],\n",
    "    })\n",
    "\n",
    "    # some additional meta information such as image width and height are also required\n",
    "    annotations[\"__meta__\"].loc[filename] = [anns[\"image_width\"], anns[\"image_height\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: store and load dataset to/from disk\n",
    "\n",
    "The inference routine and data preparation process can be time-consuming. To store the prepared dataset on disk, you can use our helper function `write_json_with_pandas`. You can reload the dataset from disk using the `read_json_with_pandas` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetis.io import write_json_with_pandas, read_json_with_pandas\n",
    "\n",
    "# write the JSON-like Python dictionary to disk\n",
    "write_json_with_pandas(\n",
    "    json_like=predictions,\n",
    "    filename=\"carla_predictions.json\",\n",
    ")\n",
    "\n",
    "write_json_with_pandas(\n",
    "    json_like=annotations,\n",
    "    filename=\"carla_annotations.json\",\n",
    ")\n",
    "\n",
    "# load the dictionaries from disk\n",
    "load_predictions = read_json_with_pandas(\"carla_predictions.json\")\n",
    "load_annotations = read_json_with_pandas(\"carla_annotations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Thetis to analyze and evaluate the AI system\n",
    "\n",
    "Once your data is in the correct format, you can call Thetis with the predictions, ground truth information, and the prepared configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thetis import thetis\n",
    "\n",
    "\n",
    "result = thetis(\n",
    "   config=\"demo_config_detection.yaml\",\n",
    "   annotations=annotations,\n",
    "   predictions=predictions,\n",
    "   output_dir=\"./output\",\n",
    "   license_file_path=\"demo_license_detection.dat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the [demo configuration file](https://raw.githubusercontent.com/EFS-OpenSource/Thetis/main/examples/demo_config_detection.yaml) for this example from our repository. For detailed information on Thetis configuration, refer to the [Configuration](https://efs-opensource.github.io/Thetis/configuration.html) section.\n",
    "\n",
    "In addition to generating the report in PDF format, which we display below, Thetis also returns its findings, final rating, and recommendations for mitigation strategies as a JSON-like dictionary. We capture this dictionary as `result` and access it as follows:\n",
    "\n",
    "* `result[<aspect>]` contains a sub-dictionary with results for each aspect of the analysis, e.g. 'fairness' or 'uncertainty'.\n",
    "* `result[<aspect>]['rating_score']` contains the rating as a score from 0 to 10.\n",
    "* `result[<aspect>]['rating_enum']` contains the rating as a grade, which can be `'GOOD'`, `'MEDIUM'`, or `'BAD'`, depending on the rating score.\n",
    "* `result[<aspect>]['recommendations']` contains findings regarding possible issues and recommendations for mitigation.\n",
    "\n",
    "Note that the remaining evaluation metrics are grouped by the specified IoU thresholds, which are used to establish correspondences between predicted and ground truth objects. For example, an IoU threshold of 0.5 might be used to decide if the bounding box of a predicted object has sufficient overlap with the bounding box of a ground truth object to count the prediction as referring to that ground truth object. In the configuration file, you can specify multiple IoU thresholds that will be used for comparison in the final evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the PDF report within the current Jupyter notebook\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"./output/report.pdf\", width=800, height=1024)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
